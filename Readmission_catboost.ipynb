{
  "cells": [],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "import os,  pandas as pd,  numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\nfrom hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n\nanalysis \u003d pd.read_sas(\u0027Dataset//combined_vars.sas7bdat\u0027, encoding\u003d\u0027latin1\u0027).drop([\u0027PERSON_ID\u0027, \u0027TRR_ID\u0027], axis\u003d1)\nX \u003d analysis.drop(\u0027CODE_REHOSP\u0027, 1)\ny \u003d analysis[\u0027CODE_REHOSP\u0027].replace(2, 0)\ncat_colidx \u003d [X.columns.get_loc(col) for col in X.columns if X[col].nunique() \u003c\u003d 10]\n\n\nfor col in cat_colidx:\n    if X[X.columns[col]].dtype \u003d\u003d \u0027float64\u0027:\n        X[X.columns[col]] \u003d X[X.columns[col]].fillna(-1).astype(int)\n    else:\n        X[X.columns[col]] \u003d X[X.columns[col]].fillna(\u0027\u0027)\n\ncbc_params \u003d {\n    \u0027max_depth\u0027: hp.choice(\u0027max_depth\u0027, np.arange(2, 11)),\n    \u0027l2_leaf_reg\u0027: hp.uniform(\u0027l2_leaf_reg\u0027, 0, 100),\n    \u0027colsample_bylevel\u0027: hp.uniform(\u0027colsample_bylevel\u0027, 0.1, 1),\n    \u0027subsample\u0027: hp.uniform(\u0027subsample\u0027, 0.1, 1),\n    \u0027eta\u0027: hp.uniform(\u0027eta\u0027, 0.01, 0.1)\n}\n\ndef f_cbc(params):\n    kfold \u003d StratifiedKFold(5, True, 2019)\n    auc \u003d np.zeros(kfold.get_n_splits())\n    cbc_pred \u003d np.zeros(len(X))\n    for i, (tr_idx, val_idx) in enumerate(kfold.split(X, y)):\n        cbc \u003d CatBoostClassifier(\n            **params,\n            n_estimators\u003d2000,\n            random_state\u003d2019,\n            eval_metric\u003d\u0027AUC\u0027,\n            cat_features\u003dcat_colidx,\n            silent\u003dTrue,\n            one_hot_max_size\u003d2,\n            bootstrap_type\u003d\u0027Bernoulli\u0027,\n            boosting_type\u003d\u0027Plain\u0027,\n        )\n        clf \u003d cbc.fit(X.iloc[tr_idx], y[tr_idx], use_best_model\u003dTrue,\n                       eval_set\u003d[(X.iloc[tr_idx], y[tr_idx]), (X.iloc[val_idx], y[val_idx])],\n                       early_stopping_rounds\u003d100,\n                       verbose_eval\u003dFalse)\n        cbc_pred[val_idx] \u003d clf.predict_proba(X.iloc[val_idx])[:, 1]\n        auc[i] \u003d roc_auc_score(y[val_idx], cbc_pred[val_idx])\n        # print(\"Mean AUC(%g|%g): %.5f\" %(i, kfold.get_n_splits(), np.sum(auc)/i))\n    return {\u0027loss\u0027: -np.mean(auc).round(5), \u0027status\u0027: STATUS_OK}\ntrials \u003d Trials()\ncbc_best \u003d fmin(f_cbc, cbc_params, algo\u003dtpe.suggest, rstate\u003dnp.random.RandomState(1565), max_evals\u003d50, trials\u003dtrials)\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}